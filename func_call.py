from rich import print
from openai import OpenAI
import json
from transformers import AutoTokenizer
import json
from jinja2 import Template
import re, json
from vllm import LLM, SamplingParams
import vllm
import yaml
print("Using vLLM version:", vllm.__version__)

def report_stance(stance: int, reason: str):
    """
    stance: int (â€‘2 to +2)
    reason: str
    """
    return {"stance": stance, "reason": reason}


def load_config(path):
    return yaml.safe_load(open(path))

def load_data(path):
    with open(path) as f:
        return json.load(f)

def build_prompt(config, item):
    tpl = Template(config[config['prompt_type']])
    tpl = tpl.render(
        query=item['query'],
        context="\n".join(item['extract'])
    )
    messages = [
        {"role": "system", "content": config[config['system_type']]},
        {"role": "user", "content": tpl}
    ]
    return messages

def extract_stance_reason(output: str):
    # Regex to extract JSON inside <tool_call> tags
    # VERSION 1
    # m = re.search(
    #     r"<tool_call>\s*({.*})\s*</tool_call>", 
    #     output, re.DOTALL)
    m = re.search(
        r"<tool_call>\s*({.*?})\s*</tool_call>",
        output,
        re.DOTALL
    )
    # m = re.findall(
    #     r"<tool_call>\s*({.*?})\s*</tool_call>",
    #     output,
    #     re.DOTALL
    # )
    if not m:
        print(ValueError("No tool_call block found"))
        return -99, -99
    try:
        raw_json = m.group(1)
        raw_json = raw_json.replace('\n', '\\n')
        raw_json = re.sub(r'("stance"\s*:\s*)\+(\d)', r'\1\2', raw_json)
        safe_json = re.sub(r'("(?:reason|.*?"\s*:\s*")(?:(?:[^"\\]|\\.)*?))\n', lambda m: m.group(1) + "\\n", raw_json)
        # safe_json = raw_json.replace('\n', '\\n')
        obj = json.loads(safe_json, strict=False)
        
        # obj = json.loads(m[0])
        args = obj["arguments"]
        return args["stance"], args["reason"]
    except:
        print("Error parsing - \n", output)
        return -99, -99
    
    

tools = [
    {
        "type": "function",
        "function": {
            "name": "report_stance",
            "description": "Report the model's stance on the query from -2 (strongly against) to +2 (strongly for)",
            "parameters": {
                "type": "object",
                "properties": {
                    "stance": {
                        "type": "integer",
                        "minimum": -2,
                        "maximum": 2,
                        "description": "Numeric stance value"
                    },
                    "reason": {
                        "type": "string",
                        "description": "Reasoning for the stance"
                    }
                },
                "required": ["stance", "reason"]
            }
        }
    }
]

model = "/cluster/scratch/sharaj/Qwen3-32B"  # Path to the model directory
# llm = LLM(model=model, disable_cascade_attn=True)#, max_model_len=16384)
llm = LLM(model=model, max_model_len=16384, disable_cascade_attn=True)

THINKING = False
sampling_params = SamplingParams(
    temperature=0.1,
    top_p=0.95,
    max_tokens=2048,
)


config = load_config("config.yaml")
data = load_data(config['eval_file'])

messages = [build_prompt(config, item) for item in data]
print("Length of messages: ", len(messages))
batch_size = 5
batched_responses = []


print("Starting generation...")
for i in range(0, len(messages), batch_size):
    batch = messages[i:i + batch_size]
    responses = llm.chat(
        batch,
        sampling_params,
        tools=tools,
        chat_template_kwargs={"enable_thinking": THINKING}
    )
    batched_responses.extend(responses)
print(f"Length of responses = {len(responses)}")

analysis = []
for response, input_ in zip(batched_responses, data):
    prompt_tokens = len(response.prompt_token_ids)
    generated_tokens = len(response.outputs[0].token_ids)
    # print(f"Prompt token length: {prompt_tokens}")
    # print(f"Generated token length: {generated_tokens}")

    out = response.outputs[0]
    stance, reason = extract_stance_reason(out.text)
    study = {"query": input_['query'], "human_score": input_['score'], "gen_score": stance, "gen_reason": reason, "comment": input_['comment']}
    analysis.append(study)

# save analysis to json file
import json
with open('analysis.json', 'w', encoding='utf-8') as f:
    json.dump(analysis, f, ensure_ascii=False, indent=4)

# quickly print accuracy
# iterate over the analysis array, compare gen_score and human_score and calculate accuracy
# also list how many answers were not generated by counting -99
accuracy = []
hit_rate = [] # if the signs are same for + and - stance, for 0 it must be zero
two_acc = []
one_acc = []
zero_acc = []
minus_one_acc = []
minus_two_acc = []
bad_gen = 0
for gen in analysis:
    if gen['gen_score'] == gen['human_score'] and gen['gen_score'] == 0:
        hit_rate.append(True)
    if (int(gen['gen_score']))*(int(gen['human_score'])) > 0:
        hit_rate.append(True)
    else:
        hit_rate.append(False)
    if gen['gen_score'] == gen['human_score']:
        accuracy.append(True)
        if gen['gen_score'] == 2:
            two_acc.append(True)
        elif gen['gen_score'] == 1:
            one_acc.append(True)
        elif gen['gen_score'] == 0:
            zero_acc.append(True)
        elif gen['gen_score'] == -1:
            minus_one_acc.append(True)
        elif gen['gen_score'] == -2:
            minus_two_acc.append(True)
    else:
        accuracy.append(False)
        if gen['gen_score'] == 2:
            two_acc.append(False)
        elif gen['gen_score'] == 1:
            one_acc.append(False)
        elif gen['gen_score'] == 0:
            zero_acc.append(False)
        elif gen['gen_score'] == -1:
            minus_one_acc.append(False)
        elif gen['gen_score'] == -2:
            minus_two_acc.append(False)
    
    if gen['gen_score'] == -99:
        bad_gen+=1

print(f"------- {config['system_type']} --------")
print(f"Model - {model.split('/')[-1]}")
print("Temperature - ", sampling_params.temperature)
print("Data for eval - ", len(messages))
print(f"Thinking - {THINKING}")  
print("-----------------------------------------")
print(f"Accuracy: {sum(accuracy)/len(accuracy)}")
print(f"Hit Rate: {sum(hit_rate)/len(hit_rate)}")
print("-----------------------------------------")
print(f"Bad Gen: {bad_gen} out of {len(accuracy)}")
print("-----------------------------------------")
print(f"Stance 2: {sum(two_acc)/len(two_acc)}")
print(f"Stance 1: {sum(one_acc)/len(one_acc)}")
print(f"Stance 0: {sum(zero_acc)/len(zero_acc)}")
print(f"Stance -1: {sum(minus_one_acc)/len(minus_one_acc)}")
print(f"Stance -2: {sum(minus_two_acc)/len(minus_two_acc)}")
print("-----------------------------------------")

